# File: vast-ai.yml
# License: Apache v2.0

name: vast-ai

on:
  workflow_dispatch:

env:
  TMP_SCRIPT_PATH: ${{ github.workspace }}/libuipc-github-actions/.github/workflows/libuipc-build.sh
  TMP_PRINT_INFO_PATH: ${{ github.workspace }}/libuipc-github-actions/.github/workflows/print-info.sh

# Follow
# https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/adding-self-hosted-runners
# to setup a self-hosted runner.
#
# Set this before running config.py
# export RUNNER_ALLOW_RUNASROOT="1"
#
# On the vast-ai side, you will need to create a template with
# Image Tag/Path: nvidia/cuda:12.4.0-base-ubuntu22.04
# Also, make sure to check "Run interactive shell server, SSH" and "Use direct SSH connection"
# Please ensure to select an instance with CUDA version 12.4 or higher.

jobs:
  build-run:
    runs-on: my-vast-ai-runner # replace with your runner name
    timeout-minutes: 30

    steps:
      - name: check out repo
        uses: actions/checkout@v3
        with:
          submodules: recursive
          path: ${{ github.workspace }}/libuipc-github-actions

      - name: print info
        run: bash $TMP_PRINT_INFO_PATH

      - name: setup
        run: |
          bash $TMP_SCRIPT_PATH ${{ github.workspace }} setup

      - name: compile
        run: |
          bash $TMP_SCRIPT_PATH ${{ github.workspace }} compile

      - name: run
        run: |
          bash $TMP_SCRIPT_PATH ${{ github.workspace }} run
